---
name: 需求探索与改进优化（最终版）
overview: 修改 requirement_explorer 去除数量限制，改为基于 token 数的迭代生成；添加负分需求改进节点；为需求池添加 token 数字段；生成后移除最后一条需求以避免不完整。明确 input_tokens 计算细节（包括完整 prompt），并将 target_tokens 作为 max_tokens 参数传入 LLM 请求。
todos:
  - id: create_token_counter
    content: 创建 utils/token_counter.py，实现基于 deepseek_v3_tokenizer 的 token 计数功能（包括 count_text_tokens 和 count_tokens）
    status: pending
  - id: extend_config
    content: 扩展 config.py 和 config.yaml，添加 max_context_length 和 one_gen_req_token 配置（maxToken 使用现有的模型配置）
    status: pending
  - id: extend_semantic_unit
    content: 扩展 SemanticUnit 模型，添加 token_count 字段和相关方法
    status: pending
    dependencies:
      - create_token_counter
  - id: refactor_explorer
    content: 重构 requirement_explorer，明确 input_tokens 计算细节（完整 prompt），将 target_tokens 作为 max_tokens 传入 LLM，改为基于 token 数的迭代生成逻辑，去掉数量限制，生成后移除最后一条需求
    status: pending
    dependencies:
      - create_token_counter
      - extend_semantic_unit
      - extend_config
  - id: create_improver
    content: 创建 requirement_improver 函数，用于改进负分需求（使用相同的 input_tokens 计算方式和 target_tokens 传入逻辑）
    status: pending
    dependencies:
      - create_token_counter
      - extend_semantic_unit
      - extend_config
  - id: create_improver_prompt
    content: 创建 requirement_improver 的提示词模板（中英文）
    status: pending
  - id: update_prompt_templates
    content: 更新 requirement_explorer 提示词模板，移除 REQUIRED_NUM 占位符
    status: pending
  - id: update_main_flow
    content: 修改 main.py 中的主流程，集成新的 requirement_explorer 和 requirement_improver，明确 input_tokens 的计算
    status: pending
    dependencies:
      - refactor_explorer
      - create_improver
---

# 需求探索与改进优

化计划

## 概述

根据流程图和需求，需要实现以下4个主要功能：

1. 修改 `requirement_explorer` 去除数量限制，改为基于 token 数的迭代生成
2. 添加负分需求改进节点
3. 为需求池（SemanticUnit）添加 token 数字段
4. 生成后移除最后一条需求以避免不完整

## 实现步骤

### 1. 创建 token 计数工具模块

- **文件**: `srs-gen2/utils/token_counter.py` (新建)
- **功能**: 
- 使用 `deepseek_v3_tokenizer` 计算文本 token 数
- 提供 `count_text_tokens(text: str)` 函数
- 参考 `srs-gen/src/utils/token_counter.py` 的实现
- 从项目根目录或环境变量 `DEEPSEEK_TOKENIZER_DIR` 查找 tokenizer 目录

### 2. 扩展配置系统

- **文件**: `srs-gen2/config.py`
- **修改**:
- 添加 `max_context_length` 属性（从 `config.max_context_length` 读取）
- 添加 `one_gen_req_token` 属性（从 `config.one_gen_req_token` 读取）
- **注意**: `maxToken` 使用现有的 `get_component_max_tokens("requirement_explorer")` 或 `chat_max_tokens`
- **文件**: `srs-gen2/config.yaml`
- **修改**: 添加配置项
  ```yaml
          max_context_length: 128000  # 最大上下文长度
          one_gen_req_token: 4096  # 单次生成需求的目标 token 数
          # maxToken 使用现有的 openai.chat.max_tokens 或 openai.components.requirement_explorer.max_tokens
  ```




### 3. 扩展 SemanticUnit 模型

- **文件**: `srs-gen2/models.py`
- **修改**:
- 在 `__init__` 中添加 `token_count: Optional[int] = None` 参数
- 在 `to_dict` 和 `from_dict` 中支持 `token_count` 字段
- 添加方法 `calculate_token_count()` 用于计算并更新 token 数

### 4. 重构 requirement_explorer

- **文件**: `srs-gen2/srs_pipeline.py`
- **修改**: `requirement_explorer` 函数
- **移除**: `required_num` 参数
- **新增参数**: 
    - `r_base: str` (保留)
    - `negative_pool: List[SemanticUnit]` (保留)
    - `input_tokens: int` (当前 prompt 的 token 数，见下方计算细节)
    - `max_context_length: int` (最大上下文长度)
    - `one_gen_req_token: int` (单次生成需求的目标 token 数)
    - `max_token: int` (从 `config.get_component_max_tokens("requirement_explorer")` 获取)
- **input_tokens 计算细节**:

    1. 加载提示词模板 `load_prompt_template("requirement_explorer")`
    2. 准备 `negative_pool_text = "\n".join([unit.text for unit in negative_pool])`
    3. 构建完整 prompt: `prompt = template.replace("{{R_BASE}}", r_base).replace("{{NEGATIVE_POOL}}", negative_pool_text).replace("{{REQUIRED_NUM}}", "")`

    - 注意：由于不再使用 `{{REQUIRED_NUM}}`，可以移除或设为空字符串

    1. 使用 `count_text_tokens(prompt)` 计算 input_tokens
    2. 或者使用 `count_tokens([{"role": "user", "content": prompt}])` 计算（包含 chat template 的 token）

- **新逻辑**:

    1. 计算目标 token 数: 
       ```python
                                   if max_token is not None:
                                       target_tokens = min(max_context_length - input_tokens, one_gen_req_token, max_token)
                                   else:
                                       target_tokens = min(max_context_length - input_tokens, one_gen_req_token)
       ```




    1. 迭代生成（最多5次）:

    - 每次生成尽可能多的需求（不限制数量）
    - **重要**: 调用 `call_llm` 时，将 `target_tokens` 作为 `max_tokens` 参数传入
    - 解析生成的需求，去掉最后一条（避免不完整）
    - 计算未重复需求的 token 总数
    - 如果达到目标 token 数，停止迭代
    - 第5次迭代如果未达到，允许计入部分重复需求补足

    1. 为每个需求计算并设置 `token_count`
    2. 返回生成的需求列表

### 5. 创建负分需求改进节点

- **文件**: `srs-gen2/srs_pipeline.py`
- **新增函数**: `requirement_improver`
- **参数**:
    - `negative_units: List[SemanticUnit]` (负分需求列表，grade < 0)
    - `r_base: str` (需求基础文档)
    - `input_tokens: int` (当前 prompt 的 token 数，计算方式类似 requirement_explorer)
    - `max_context_length: int`
    - `one_gen_req_token: int`
    - `max_token: int` (从 `config.get_component_max_tokens("requirement_improver")` 获取，如果不存在则使用 `requirement_explorer` 的配置)
- **input_tokens 计算**:
    - 加载提示词模板 `load_prompt_template("requirement_improver")`
    - 准备负分需求文本列表
    - 构建完整 prompt 并计算 token 数
- **逻辑**: 与 `requirement_explorer` 类似的迭代生成逻辑
    - 目标 token 数: `min(max_context_length - input_tokens, one_gen_req_token, max_token)`（如果 max_token 为 None 则忽略）
    - 最多5次迭代
    - **重要**: 调用 `call_llm` 时，将 `target_tokens` 作为 `max_tokens` 参数传入
    - 生成改进后的需求，更新到需求池
    - 为每个改进后的需求计算 token 数

### 6. 修改主流程

- **文件**: `srs-gen2/main.py`
- **修改**: `run_srs_iteration` 函数
- 在 `requirement_explorer` 调用前:

    1. **计算 input_tokens**:

    - 加载 `requirement_explorer` 模板
    - 准备 `negative_pool_text`（合并 pool 和 buffer_new_units）
    - 构建完整 prompt（替换占位符）
    - 使用 `count_text_tokens(prompt)` 或 `count_tokens([{"role": "user", "content": prompt}])` 计算

    1. 获取 `max_context_length` 和 `one_gen_req_token` 配置
    2. 获取 `max_token`（使用 `config.get_component_max_tokens("requirement_explorer")`）
    3. 调用新的 `requirement_explorer`（不再使用 `required_num`）

- 在 `requirement_explorer` 调用后、`requirement_clarifier` 调用前:

    1. 筛选出负分需求（grade < 0）
    2. 如果有负分需求:

    - **计算 input_tokens**（类似 requirement_explorer 的方式）
    - 获取 `max_token`（使用 `config.get_component_max_tokens("requirement_improver")` 或回退到 `requirement_explorer` 的配置）
    - 调用 `requirement_improver` 进行改进

    1. 将改进后的需求合并到需求池

### 7. 更新提示词模板

- **文件**: `srs-gen2/prompts/*/requirement_explorer.md`
- **修改**: 
- 移除 `{{REQUIRED_NUM}}` 占位符及相关说明
- 改为提示生成尽可能多的需求，不限制数量

### 8. 创建需求改进提示词模板

- **文件**: `srs-gen2/prompts/*/requirement_improver.md` (新建)
- **内容**: 提示 LLM 改进负分需求，使其更符合基准 SRS
- **占位符**: 
- `{{R_BASE}}`: 需求基础文档
- `{{NEGATIVE_UNITS}}`: 需要改进的负分需求列表（每行一个，格式：`REQ: 需求文本`）

## 关键实现细节

### input_tokens 计算详细说明

```python
# 方式1: 仅计算 prompt 文本的 token（不包含 chat template）
from utils.token_counter import count_text_tokens
template = load_prompt_template("requirement_explorer")
negative_pool_text = "\n".join([unit.text for unit in negative_pool])
prompt = template.replace("{{R_BASE}}", r_base).replace("{{NEGATIVE_POOL}}", negative_pool_text)
input_tokens = count_text_tokens(prompt)

# 方式2: 计算包含 chat template 的完整 token（更准确）
from utils.token_counter import count_tokens
messages = [{"role": "user", "content": prompt}]
input_tokens = count_tokens(messages)  # 包含 role tokens 等
```

**推荐使用方式2**，因为它更准确地反映了实际发送给 LLM 的 token 数。

### target_tokens 传入 LLM

```python
# 计算 target_tokens
if max_token is not None:
    target_tokens = min(max_context_length - input_tokens, one_gen_req_token, max_token)
else:
    target_tokens = min(max_context_length - input_tokens, one_gen_req_token)

# 调用 LLM 时传入
response = call_llm(
    prompt,
    model=model,
    temperature=temperature,
    max_tokens=target_tokens,  # 使用计算出的 target_tokens
    max_continuations=max_continuations,
)
```



### 迭代逻辑

```python
target_tokens = min(max_context_length - input_tokens, one_gen_req_token, max_token) if max_token else min(max_context_length - input_tokens, one_gen_req_token)
current_tokens = 0
unique_units = []
seen_texts = set()

for iteration in range(5):
    # 调用 LLM，传入 target_tokens 作为 max_tokens
    generated = requirement_explorer_single_call(
        r_base, negative_pool, target_tokens, ...
    )
    generated = generated[:-1]  # 去掉最后一条
    
    # 去重并计算 token
    for unit in generated:
        if unit.text not in seen_texts:
            unique_units.append(unit)
            seen_texts.add(unit.text)
            current_tokens += unit.token_count
    
    if current_tokens >= target_tokens:
        break
    
    if iteration == 4:  # 第5次
        # 允许部分重复需求补足
        remaining = target_tokens - current_tokens
        # 添加重复需求直到达到目标
        ...
```



### 负分需求改进

- 筛选 `grade < 0` 的需求
- 使用类似的迭代逻辑生成改进版本
- 改进后的需求替换原需求（或作为新需求添加）

## 文件清单

- 新建: `srs-gen2/utils/token_counter.py`
- 新建: `srs-gen2/prompts/en/requirement_improver.md`
- 新建: `srs-gen2/prompts/zh/requirement_improver.md` (如果需要中文支持)
- 修改: `srs-gen2/models.py`
- 修改: `srs-gen2/config.py`