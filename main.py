"""ä¸»ç¨‹åºå…¥å£"""

import argparse
import logging
import os
import statistics
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path

from dotenv import load_dotenv

from src.config import load_config
from src.evaluator import DocumentEvaluation, Evaluator
from src.output_formatter import OutputFormatter
from src.point_extractor import PointExtractor
import re

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


def detect_stages(srs_collection_dir: Path) -> dict[str, list[Path]]:
    """
    æ£€æµ‹srs_collectionç›®å½•ä¸‹çš„å„ä¸ªé˜¶æ®µ
    
    Args:
        srs_collection_dir: srs_collectionç›®å½•è·¯å¾„
        
    Returns:
        å­—å…¸ï¼Œé”®ä¸ºé˜¶æ®µåç§°ï¼Œå€¼ä¸ºè¯¥é˜¶æ®µä¸‹çš„æ–‡æ¡£è·¯å¾„åˆ—è¡¨
    """
    stages = {}
    
    if not srs_collection_dir.exists() or not srs_collection_dir.is_dir():
        return stages
    
    # éå†æ‰€æœ‰å­ç›®å½•
    for stage_dir in srs_collection_dir.iterdir():
        if not stage_dir.is_dir():
            continue
        
        stage_name = stage_dir.name
        # æ£€æŸ¥æ˜¯å¦æ˜¯é˜¶æ®µç›®å½•ï¼ˆsrs_document_å¼€å¤´ï¼‰
        if not stage_name.startswith("srs_document_"):
            continue
        
        # æ”¶é›†è¯¥é˜¶æ®µä¸‹çš„æ‰€æœ‰.mdæ–‡ä»¶
        md_files = sorted(stage_dir.glob("*.md")) + sorted(stage_dir.glob("*.markdown"))
        if md_files:
            stages[stage_name] = md_files
    
    return stages


def evaluate_stage(
    stage_docs: list[Path],
    baseline_path: Path | None,
    baseline_dir: Path | None,
    stage_output_dir: Path,
    config,
    args,
    judges: int,
    logger
) -> list[DocumentEvaluation]:
    """
    è¯„ä¼°å•ä¸ªé˜¶æ®µçš„æ‰€æœ‰æ–‡æ¡£
    
    Args:
        stage_docs: è¯¥é˜¶æ®µçš„æ–‡æ¡£è·¯å¾„åˆ—è¡¨
        baseline_path: åŸºå‡†æ–‡æ¡£è·¯å¾„ï¼ˆå•ä¸ªåŸºå‡†æ–‡æ¡£æ¨¡å¼ï¼‰
        baseline_dir: åŸºå‡†æ–‡æ¡£ç›®å½•ï¼ˆåŒ¹é…æ¨¡å¼ï¼‰
        stage_output_dir: è¯¥é˜¶æ®µçš„è¾“å‡ºç›®å½•
        config: é…ç½®å¯¹è±¡
        args: å‘½ä»¤è¡Œå‚æ•°
        judges: è¯„å§”æ•°é‡
        logger: æ—¥å¿—è®°å½•å™¨
        
    Returns:
        è¯¥é˜¶æ®µçš„è¯„ä¼°ç»“æœåˆ—è¡¨
    """
    formatter = OutputFormatter()
    use_matching_mode = baseline_dir is not None
    
    # å¦‚æœä¸æ˜¯åŒ¹é…æ¨¡å¼ï¼Œä»åŸºå‡†æ–‡æ¡£æå–è¦ç‚¹æ¸…å•
    checkpoints = None
    if not use_matching_mode and baseline_path:
        logger.info(f"æ­£åœ¨ä»åŸºå‡†æ–‡æ¡£æå–è¦ç‚¹æ¸…å•: {baseline_path}")
        logger.info("-" * 60)
        try:
            extractor = PointExtractor(config, prompt_version=config.prompt_version)
            checkpoints = extractor.extract_points(
                baseline_path,
                force_extract=args.force_extract,
                extract_runs=args.extract_runs,
            )
            logger.info(f"âœ“ æ£€æŸ¥é¡¹æ¸…å•ï¼šå…± {len(checkpoints)} ä¸ªæ£€æŸ¥é¡¹")
            logger.info("")
        except Exception as e:
            logger.error(f"æå–è¦ç‚¹å¤±è´¥: {e}")
            logger.debug(f"è°ƒè¯•ä¿¡æ¯:", exc_info=True)
            return []
    elif use_matching_mode:
        extractor = PointExtractor(config, prompt_version=config.prompt_version)
    
    # æ£€æŸ¥å·²å­˜åœ¨çš„è¯„ä¼°ç»“æœ
    evaluations = []
    new_target_paths = []
    
    if args.skip_existing:
        force_re_eval_set = set()
        if args.force_re_eval:
            for item in args.force_re_eval:
                item_path = Path(item)
                force_re_eval_set.add(item_path.stem)
        
        for target_path in stage_docs:
            doc_name = Path(target_path).stem
            
            if args.force_re_eval and doc_name in force_re_eval_set:
                logger.info(f"ğŸ”„ {doc_name} - å¼ºåˆ¶é‡æ–°è¯„ä¼°")
                new_target_paths.append(target_path)
                continue
            
            json_path = stage_output_dir / f"{doc_name}_evaluation.json"
            if json_path.exists():
                existing_eval = formatter.load_json(json_path)
                if existing_eval:
                    evaluations.append(existing_eval)
                    logger.info(f"âŠ˜ {doc_name} - å·²å­˜åœ¨ï¼Œè·³è¿‡è¯„ä¼°")
                    continue
            
            new_target_paths.append(target_path)
        
        if evaluations:
            logger.info(f"å·²è·³è¿‡ {len(evaluations)} ä¸ªå·²å­˜åœ¨çš„è¯„ä¼°ç»“æœ")
        if new_target_paths:
            logger.info(f"éœ€è¦è¯„ä¼° {len(new_target_paths)} ä¸ªæ–°æ–‡æ¡£")
        logger.info("")
    else:
        new_target_paths = stage_docs
    
    # è¯„ä¼°æ–‡æ¡£
    evaluator = Evaluator(config, prompt_version=config.prompt_version)
    
    parallel_eval = len(new_target_paths) > 1
    max_workers = args.max_workers
    if parallel_eval and new_target_paths:
        if max_workers is None:
            max_workers = min(len(new_target_paths), 10)
        logger.info(f"â„¹ å¹¶è¡Œæ‰§è¡Œæ¨¡å¼ï¼šæœ€å¤§å·¥ä½œçº¿ç¨‹æ•° = {max_workers}")
        logger.info("")
    
    def evaluate_document(target_path: Path) -> tuple[Path, DocumentEvaluation | None]:
        """è¯„ä¼°å•ä¸ªæ–‡æ¡£çš„å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œæ‰§è¡Œ"""
        try:
            doc_baseline_path = baseline_path
            doc_checkpoints = checkpoints
            
            if use_matching_mode:
                matched_baseline = find_matching_baseline(target_path, baseline_dir)
                if matched_baseline is None:
                    logger.warning(f"æœªæ‰¾åˆ° {target_path.name} çš„åŒ¹é…åŸºå‡†æ–‡æ¡£ï¼Œè·³è¿‡è¯„ä¼°")
                    return (target_path, None)
                doc_baseline_path = matched_baseline
                
                try:
                    doc_checkpoints = extractor.extract_points(
                        doc_baseline_path,
                        force_extract=args.force_extract,
                        extract_runs=args.extract_runs,
                    )
                except Exception as e:
                    logger.error(f"ä»åŸºå‡†æ–‡æ¡£ {doc_baseline_path.name} æå–è¦ç‚¹å¤±è´¥: {e}")
                    return (target_path, None)
            
            if judges > 1:
                evaluation = evaluator.evaluate_multiple_runs(
                    doc_checkpoints, target_path, runs=judges, baseline_document_path=doc_baseline_path
                )
            else:
                start_time = time.time()
                evaluation_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                evaluation = evaluator.evaluate_single_run(doc_checkpoints, target_path)
                evaluation.model_name = config.openai.model
                evaluation.baseline_document = str(doc_baseline_path)
                evaluation.evaluation_time = evaluation_time
                evaluation.evaluation_duration = time.time() - start_time
            return (target_path, evaluation)
        except Exception as e:
            logger.error(f"è¯„ä¼°æ–‡æ¡£ {target_path} å¤±è´¥: {e}")
            logger.debug(f"è¯„ä¼°å¤±è´¥è¯¦æƒ…:", exc_info=True)
            return (target_path, None)
    
    # æ‰§è¡Œè¯„ä¼°
    if parallel_eval and new_target_paths:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_path = {executor.submit(evaluate_document, path): path for path in new_target_paths}
            completed = 0
            for future in as_completed(future_to_path):
                completed += 1
                target_path, evaluation = future.result()
                if evaluation:
                    evaluations.append(evaluation)
                    doc_name = target_path.stem
                    weighted_score = OutputFormatter._calculate_weighted_score(evaluation)
                    logger.info(f"[{completed}/{len(new_target_paths)}] âœ“ {doc_name}: åŠ æƒå¾—åˆ†={weighted_score:.2f}")
                else:
                    logger.error(f"[{completed}/{len(new_target_paths)}] âœ— {target_path.stem}: è¯„ä¼°å¤±è´¥")
    else:
        for target_path in new_target_paths:
            target_path, evaluation = evaluate_document(target_path)
            if evaluation:
                evaluations.append(evaluation)
                doc_name = target_path.stem
                weighted_score = OutputFormatter._calculate_weighted_score(evaluation)
                logger.info(f"âœ“ {doc_name}: åŠ æƒå¾—åˆ†={weighted_score:.2f}")
            else:
                logger.error(f"âœ— {target_path.stem}: è¯„ä¼°å¤±è´¥")
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    for evaluation in evaluations:
        doc_name = Path(evaluation.target_document).stem
        
        json_path = stage_output_dir / f"{doc_name}_evaluation.json"
        formatter.save_json(evaluation, json_path)
        
        if args.output in ["markdown", "all"]:
            md_path = stage_output_dir / f"{doc_name}_evaluation.md"
            formatter.save_markdown(evaluation, md_path)
        
        tsv_path = stage_output_dir / f"{doc_name}_evaluation.tsv"
        formatter.save_tsv(evaluation, tsv_path)
    
    # ç”ŸæˆCSVæ±‡æ€»
    if args.output in ["csv", "all"] and evaluations:
        csv_path = stage_output_dir / "evaluations_summary.csv"
        formatter.to_csv(evaluations, csv_path)
        logger.info(f"âœ“ CSV: {csv_path}")
    
    # ç”Ÿæˆé˜¶æ®µèšåˆæŠ¥å‘Š
    if len(evaluations) > 1:
        logger.info("")
        logger.info("æ­£åœ¨ç”Ÿæˆé˜¶æ®µèšåˆç»Ÿè®¡æŠ¥å‘Š...")
        summary_path = stage_output_dir / "summary_report.md"
        total_time = sum(
            e.evaluation_duration for e in evaluations 
            if e.evaluation_duration is not None
        )
        
        target_dir_path = None
        baseline_dir_path = None
        if use_matching_mode:
            baseline_dir_path = baseline_dir
        
        formatter.save_summary_report(
            evaluations,
            summary_path,
            baseline_path,
            target_dir=target_dir_path,
            baseline_dir=baseline_dir_path,
            output_dir=stage_output_dir,
            judges=judges,
            total_time=total_time,
        )
        logger.info(f"âœ“ é˜¶æ®µèšåˆç»Ÿè®¡æŠ¥å‘Š: {summary_path}")
    
    return evaluations


def sort_stage_names(stage_names: list[str]) -> list[str]:
    """
    å¯¹é˜¶æ®µåç§°è¿›è¡Œæ’åº
    
    æ’åºè§„åˆ™ï¼š
    1. no-explore-clarify
    2. no-clarify
    3. iter1, iter2, iter3, ...
    
    Args:
        stage_names: é˜¶æ®µåç§°åˆ—è¡¨
        
    Returns:
        æ’åºåçš„é˜¶æ®µåç§°åˆ—è¡¨
    """
    def stage_key(name: str) -> tuple:
        """ç”Ÿæˆæ’åºé”®"""
        if name == "srs_document_no-explore-clarify":
            return (0, 0)
        elif name == "srs_document_no-clarify":
            return (1, 0)
        elif name.startswith("srs_document_iter"):
            # æå–è¿­ä»£æ¬¡æ•°
            match = re.search(r'iter(\d+)', name)
            if match:
                return (2, int(match.group(1)))
            return (2, 999)  # æ— æ³•è§£æçš„iteræ”¾åœ¨æœ€å
        else:
            return (3, 0)  # å…¶ä»–é˜¶æ®µæ”¾åœ¨æœ€å
    
    return sorted(stage_names, key=stage_key)

# é…ç½®æ—¥å¿— - ä»ç¯å¢ƒå˜é‡è¯»å–æ—¥å¿—çº§åˆ«å’Œæ–‡ä»¶è·¯å¾„
def get_log_level():
    """ä»ç¯å¢ƒå˜é‡è·å–æ—¥å¿—çº§åˆ«ï¼Œé»˜è®¤ä¸ºINFO"""
    log_level_str = os.getenv("LOG_LEVEL", "INFO").upper()
    log_levels = {
        "DEBUG": logging.DEBUG,
        "INFO": logging.INFO,
        "WARNING": logging.WARNING,
        "ERROR": logging.ERROR,
        "CRITICAL": logging.CRITICAL,
    }
    return log_levels.get(log_level_str, logging.INFO)


def setup_logging():
    """é…ç½®æ—¥å¿—ç³»ç»Ÿï¼Œæ§åˆ¶å°æ˜¾ç¤ºè¿›åº¦å’Œç®€çŸ­å¼‚å¸¸ï¼Œæ–‡ä»¶è®°å½•å®Œæ•´æ—¥å¿—
    
    æ§åˆ¶å°æ—¥å¿—çº§åˆ«ä¸ºINFOï¼Œåªæ˜¾ç¤ºè¿›åº¦ã€ç»“æœå’Œå¼‚å¸¸ä¿¡æ¯ã€‚
    æ–‡ä»¶æ—¥å¿—çº§åˆ«ä¸ºDEBUGï¼Œè®°å½•æ‰€æœ‰è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬å®Œæ•´çš„æ¨¡å‹äº¤äº’æ¶ˆæ¯ï¼‰ã€‚
    è¯¦ç»†çš„æ¨¡å‹äº¤äº’æ¶ˆæ¯ä½¿ç”¨DEBUGçº§åˆ«ï¼Œå› æ­¤ä¸ä¼šæ˜¾ç¤ºåœ¨æ§åˆ¶å°ã€‚
    """
    from datetime import datetime
    
    # æ–‡ä»¶æ—¥å¿—çº§åˆ«ï¼šä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤ä¸ºDEBUGä»¥è®°å½•å®Œæ•´ä¿¡æ¯
    file_log_level_str = os.getenv("LOG_LEVEL", "DEBUG").upper()
    file_log_levels = {
        "DEBUG": logging.DEBUG,
        "INFO": logging.INFO,
        "WARNING": logging.WARNING,
        "ERROR": logging.ERROR,
        "CRITICAL": logging.CRITICAL,
    }
    file_log_level = file_log_levels.get(file_log_level_str, logging.DEBUG)
    
    # æ§åˆ¶å°æ—¥å¿—çº§åˆ«ï¼šINFOï¼Œåªæ˜¾ç¤ºè¿›åº¦ã€ç»“æœå’Œå¼‚å¸¸ä¿¡æ¯
    # DEBUGçº§åˆ«çš„è¯¦ç»†äº¤äº’æ¶ˆæ¯ä¸ä¼šæ˜¾ç¤ºåœ¨æ§åˆ¶å°
    console_log_level = logging.INFO
    
    # è·å–æ ¹loggerï¼Œè®¾ç½®ä¸ºæœ€ä½çº§åˆ«ï¼ˆDEBUGï¼‰ä»¥å…è®¸æ‰€æœ‰æ—¥å¿—é€šè¿‡
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    
    # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
    root_logger.handlers.clear()
    
    # æ§åˆ¶å°æ ¼å¼åŒ–å™¨ï¼šç®€æ´æ ¼å¼ï¼ˆåªæ˜¾ç¤ºæ¶ˆæ¯ï¼Œä¸æ˜¾ç¤ºçº§åˆ«ï¼‰
    console_formatter = logging.Formatter('%(message)s')
    
    # æ–‡ä»¶æ ¼å¼åŒ–å™¨ï¼šå®Œæ•´æ ¼å¼ï¼ˆåŒ…å«æ—¶é—´æˆ³ã€æ¨¡å—åã€çº§åˆ«ã€æ¶ˆæ¯ï¼‰
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨ï¼ˆINFOçº§åˆ«ï¼‰
    # DEBUGçº§åˆ«çš„æ—¥å¿—ä¸ä¼šæ˜¾ç¤ºåœ¨æ§åˆ¶å°ï¼Œå› ä¸ºçº§åˆ«ä½äºINFO
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(console_log_level)
    console_handler.setFormatter(console_formatter)
    root_logger.addHandler(console_handler)
    
    # å¤„ç†æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒæ—¶é—´æˆ³å ä½ç¬¦ï¼‰
    log_file = os.getenv("LOG_FILE")
    if log_file:
        # æ›¿æ¢æ—¶é—´æˆ³å ä½ç¬¦
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = log_file.replace('{timestamp}', timestamp)
        log_file = log_file.replace('{datetime}', timestamp)
        log_file = log_file.replace('{date}', datetime.now().strftime('%Y%m%d'))
        log_file = log_file.replace('{time}', datetime.now().strftime('%H%M%S'))
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        log_path = Path(log_file)
        log_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä½¿ç”¨è¿½åŠ æ¨¡å¼
        file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
        file_handler.setLevel(file_log_level)
        file_handler.setFormatter(file_formatter)
        # æ–‡ä»¶å¤„ç†å™¨è®°å½•æ‰€æœ‰è¯¦ç»†ä¿¡æ¯ï¼ˆDEBUGçº§åˆ«ï¼‰
        root_logger.addHandler(file_handler)
        # ä½¿ç”¨logger.infoç¡®ä¿è¿™ä¸ªæ¶ˆæ¯ä¼šæ˜¾ç¤ºåœ¨æ§åˆ¶å°
        logger = logging.getLogger(__name__)
        logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file} (çº§åˆ«: {logging.getLevelName(file_log_level)})")
    else:
        logger = logging.getLogger(__name__)
        # ä¸è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œåªåœ¨æ–‡ä»¶ä¸­è®°å½•
        logger.debug("æ—¥å¿—ä»…è¾“å‡ºåˆ°æ§åˆ¶å°ï¼ˆè®¾ç½®LOG_FILEç¯å¢ƒå˜é‡å¯åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶ï¼‰")
    
    # ç¦ç”¨ç¬¬ä¸‰æ–¹åº“çš„è¯¦ç»†æ—¥å¿—ï¼ˆåªæ˜¾ç¤ºWARNINGåŠä»¥ä¸Šçº§åˆ«ï¼‰
    # è¿™äº›åº“ä¼šäº§ç”Ÿå¤§é‡DEBUGå’ŒINFOçº§åˆ«çš„HTTPè¯·æ±‚æ—¥å¿—
    logging.getLogger("openai").setLevel(logging.WARNING)
    logging.getLogger("httpcore").setLevel(logging.WARNING)
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    
    return logger

logger = setup_logging()


def find_matching_baseline(target_file: Path, baseline_dir: Path) -> Path | None:
    """æ ¹æ®ç›®æ ‡æ–‡æ¡£æ–‡ä»¶åæŸ¥æ‰¾åŒ¹é…çš„åŸºå‡†æ–‡æ¡£
    
    Args:
        target_file: ç›®æ ‡æ–‡æ¡£è·¯å¾„
        baseline_dir: åŸºå‡†æ–‡æ¡£ç›®å½•
        
    Returns:
        åŒ¹é…çš„åŸºå‡†æ–‡æ¡£è·¯å¾„ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
    """
    target_stem = target_file.stem  # ä¸å«æ‰©å±•åçš„æ–‡ä»¶å
    
    # 1. å®Œå…¨åŒ¹é…æ–‡ä»¶åï¼ˆå¿½ç•¥æ‰©å±•åï¼‰
    for ext in ['.md', '.txt', '.markdown']:
        baseline_path = baseline_dir / f"{target_stem}{ext}"
        if baseline_path.exists() and baseline_path.is_file():
            return baseline_path
    
    # 2. å°è¯•åœ¨åŸºå‡†ç›®å½•ä¸­æŸ¥æ‰¾åŒ…å«ç›®æ ‡æ–‡ä»¶åçš„æ–‡ä»¶
    for baseline_file in baseline_dir.glob("*"):
        if baseline_file.is_file():
            baseline_stem = baseline_file.stem
            # å¦‚æœåŸºå‡†æ–‡ä»¶ååŒ…å«ç›®æ ‡æ–‡ä»¶åï¼Œæˆ–è€…ç›®æ ‡æ–‡ä»¶ååŒ…å«åŸºå‡†æ–‡ä»¶å
            if target_stem in baseline_stem or baseline_stem in target_stem:
                return baseline_file
    
    return None


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="éœ€æ±‚æ–‡æ¡£å·®å¼‚è¯„ä¼°ç³»ç»Ÿ - åŸºäºå¤§æ¨¡å‹è¯„ä¼°éœ€æ±‚æ–‡æ¡£"
    )
    parser.add_argument(
        "--baseline",
        type=str,
        help="åŸºå‡†æ–‡æ¡£è·¯å¾„ï¼ˆä½œä¸ºçœŸå€¼ï¼Œå•ä¸ªæ–‡ä»¶ï¼‰",
    )
    parser.add_argument(
        "--baseline-dir",
        type=str,
        help="åŸºå‡†æ–‡æ¡£æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆä½œä¸ºçœŸå€¼ï¼Œæ–‡ä»¶å¤¹ä¸­çš„ç¬¬ä¸€ä¸ª .md æ–‡ä»¶ï¼‰",
    )
    parser.add_argument(
        "--target",
        type=str,
        help="å¾…è¯„ä¼°æ–‡æ¡£è·¯å¾„ï¼ˆå•ä¸ªæ–‡æ¡£ï¼‰",
    )
    parser.add_argument(
        "--targets",
        type=str,
        nargs="+",
        help="å¾…è¯„ä¼°æ–‡æ¡£è·¯å¾„ï¼ˆå¤šä¸ªæ–‡æ¡£ï¼‰",
    )
    parser.add_argument(
        "--target-dir",
        type=str,
        help="å¾…è¯„ä¼°æ–‡æ¡£æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆæ‰¹é‡è¯„ä¼°æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ .md æ–‡ä»¶ï¼‰",
    )
    parser.add_argument(
        "--srs-collection-dir",
        type=str,
        help="srs_collectionç›®å½•è·¯å¾„ï¼ˆè‡ªåŠ¨æŒ‰é˜¶æ®µåˆ†ç»„è¯„ä¼°ï¼Œæ¯ä¸ªé˜¶æ®µç”Ÿæˆç‹¬ç«‹æŠ¥å‘Šï¼‰",
    )
    parser.add_argument(
        "--judges",
        type=int,
        default=None,
        help="è¯„å§”æ•°é‡ï¼Œæ¯æ¬¡è¯„ä¼°ä¼šè¿è¡ŒæŒ‡å®šæ¬¡æ•°ï¼Œç„¶åä½¿ç”¨åˆå¹¶ç­–ç•¥ï¼ˆå¦‚å¤šæ•°æŠ•ç¥¨ï¼‰åˆå¹¶ç»“æœã€‚é»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼",
    )
    parser.add_argument(
        "--output",
        type=str,
        choices=["json", "csv", "markdown", "all"],
        default="markdown",
        help="è¾“å‡ºæ ¼å¼ï¼ˆé»˜è®¤ï¼šmarkdownï¼‰",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="output",
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šoutputï¼‰",
    )
    parser.add_argument(
        "--force-extract",
        action="store_true",
        help="å¼ºåˆ¶é‡æ–°æå–è¦ç‚¹æ¸…å•ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰",
    )
    parser.add_argument(
        "--extract-runs",
        type=int,
        default=1,
        help="æå–è¦ç‚¹æ¸…å•çš„è¿è¡Œæ¬¡æ•°ï¼Œå¤šæ¬¡æå–åé€‰æ‹©æ£€æŸ¥é¡¹æ•°é‡æœ€å¤šçš„ç»“æœï¼ˆé»˜è®¤ï¼š1ï¼‰",
    )
    parser.add_argument(
        "--max-workers",
        type=int,
        default=None,
        help="å¹¶è¡Œæ‰§è¡Œçš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤ï¼šè‡ªåŠ¨ï¼Œæ ¹æ®ä»»åŠ¡æ•°é‡è°ƒæ•´ï¼‰",
    )
    parser.add_argument(
        "--extract-cache-only",
        action="store_true",
        help="ä»…æ„å»ºè¦ç‚¹ç¼“å­˜ï¼Œä¸è¿›è¡Œè¯„ä¼°",
    )
    parser.add_argument(
        "--prompt-version",
        type=str,
        default=None,
        help="æç¤ºè¯ç‰ˆæœ¬ï¼ˆé»˜è®¤ï¼šä»ç¯å¢ƒå˜é‡PROMPT_VERSIONæˆ–é…ç½®ä¸­è¯»å–ï¼Œé»˜è®¤å€¼ä¸ºv1ï¼‰",
    )
    parser.add_argument(
        "--skip-existing",
        action="store_true",
        help="è·³è¿‡å·²å­˜åœ¨çš„è¯„ä¼°ç»“æœï¼ˆå¦‚æœè¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™åŠ è½½å·²æœ‰ç»“æœè€Œä¸æ˜¯é‡æ–°è¯„ä¼°ï¼‰",
    )
    parser.add_argument(
        "--force-re-eval",
        type=str,
        nargs="+",
        help="å¼ºåˆ¶é‡æ–°è¯„ä¼°æŒ‡å®šçš„æ–‡æ¡£ï¼ˆå³ä½¿å·²å­˜åœ¨è¯„ä¼°ç»“æœï¼Œä¹Ÿä¼šé‡æ–°è¯„ä¼°ï¼‰ã€‚å¯ä»¥æŒ‡å®šæ–‡æ¡£åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰æˆ–å®Œæ•´è·¯å¾„",
    )

    args = parser.parse_args()

    # éªŒè¯å‚æ•°
    if not args.baseline and not args.baseline_dir:
        parser.error("å¿…é¡»æŒ‡å®š --baseline æˆ– --baseline-dir")
    
    # å¦‚æœä½¿ç”¨ --extract-cache-onlyï¼Œå¯ä»¥ä¸æŒ‡å®š target
    if not args.extract_cache_only:
        if not args.target and not args.targets and not args.target_dir and not args.srs_collection_dir:
            parser.error("å¿…é¡»æŒ‡å®š --targetã€--targetsã€--target-dir æˆ– --srs-collection-dir")

    # åŠ è½½é…ç½®
    try:
        config = load_config()
        # å¦‚æœé€šè¿‡å‚æ•°æŒ‡å®šäº†æç¤ºè¯ç‰ˆæœ¬ï¼Œè¦†ç›–é…ç½®ä¸­çš„ç‰ˆæœ¬
        if args.prompt_version:
            config.prompt_version = args.prompt_version
    except ValueError as e:
        logger.error(f"é…ç½®é”™è¯¯: {e}")
        sys.exit(1)

    # è®°å½•æ€»å¼€å§‹æ—¶é—´
    total_start_time = time.time()

    # ç¡®å®šè¯„å§”æ•°é‡
    judges = args.judges if args.judges is not None else config.eval.default_runs

    # ç¡®å®šåŸºå‡†æ–‡æ¡£æˆ–åŸºå‡†æ–‡æ¡£ç›®å½•
    baseline_path = None
    baseline_dir = None
    use_matching_mode = False  # æ˜¯å¦ä½¿ç”¨åŒ¹é…æ¨¡å¼ï¼ˆä¸ºæ¯ä¸ªç›®æ ‡æ–‡æ¡£åŒ¹é…å¯¹åº”çš„åŸºå‡†æ–‡æ¡£ï¼‰
    
    if args.baseline:
        baseline_path = Path(args.baseline)
        if not baseline_path.exists():
            logger.error(f"åŸºå‡†æ–‡æ¡£ä¸å­˜åœ¨: {args.baseline}")
            sys.exit(1)
        if not baseline_path.is_file():
            logger.error(f"åŸºå‡†æ–‡æ¡£è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {args.baseline}")
            sys.exit(1)
    elif args.baseline_dir:
        baseline_dir = Path(args.baseline_dir)
        if not baseline_dir.exists():
            logger.error(f"åŸºå‡†æ–‡æ¡£æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {args.baseline_dir}")
            sys.exit(1)
        if not baseline_dir.is_dir():
            logger.error(f"åŸºå‡†æ–‡æ¡£è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {args.baseline_dir}")
            sys.exit(1)
        # æ£€æŸ¥æ˜¯å¦åŒæ—¶æŒ‡å®šäº†ç›®æ ‡æ–‡æ¡£ç›®å½•ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™ä½¿ç”¨åŒ¹é…æ¨¡å¼
        if args.target_dir:
            use_matching_mode = True
            logger.info(f"ä½¿ç”¨åŒ¹é…æ¨¡å¼ï¼šå°†ä¸ºæ¯ä¸ªç›®æ ‡æ–‡æ¡£åŒ¹é…å¯¹åº”çš„åŸºå‡†æ–‡æ¡£")
        elif not args.extract_cache_only:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®æ ‡æ–‡æ¡£ç›®å½•ï¼Œä¸”ä¸æ˜¯ä»…æ„å»ºç¼“å­˜æ¨¡å¼ï¼Œåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ª .md æ–‡ä»¶ä½œä¸ºåŸºå‡†æ–‡æ¡£
            md_files = sorted(baseline_dir.glob("*.md")) + sorted(baseline_dir.glob("*.markdown"))
            if not md_files:
                logger.error(f"åŸºå‡†æ–‡æ¡£æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ° .md æ–‡ä»¶: {args.baseline_dir}")
                sys.exit(1)
            baseline_path = md_files[0]
            logger.info(f"ä»åŸºå‡†æ–‡æ¡£æ–‡ä»¶å¤¹ä¸­é€‰æ‹©: {baseline_path.name}")

    # å¦‚æœä½¿ç”¨ --extract-cache-onlyï¼Œä»…æ„å»ºè¦ç‚¹ç¼“å­˜ï¼Œä¸è¿›è¡Œè¯„ä¼°
    if args.extract_cache_only:
        logger.info("=" * 60)
        logger.info("ä»…æ„å»ºè¦ç‚¹ç¼“å­˜æ¨¡å¼ï¼ˆä¸è¿›è¡Œè¯„ä¼°ï¼‰")
        logger.info("=" * 60)
        logger.info("")
        
        extractor = PointExtractor(config, prompt_version=config.prompt_version)
        logger.info(f"ä½¿ç”¨æ¨¡å‹: {config.openai.model}")
        logger.info(f"APIåœ°å€: {config.openai.base_url}")
        if args.prompt_version:
            logger.info(f"æç¤ºè¯ç‰ˆæœ¬: {config.prompt_version}")
        
        if args.force_extract:
            logger.info("âš  å¼ºåˆ¶é‡æ–°æå–æ¨¡å¼ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰")
        else:
            logger.info("â„¹ ä½¿ç”¨ç¼“å­˜æœºåˆ¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰")
        
        if args.extract_runs > 1:
            logger.info(f"â„¹ å¤šæ¬¡æå–æ¨¡å¼ï¼šå°†æ‰§è¡Œ {args.extract_runs} æ¬¡æå–ï¼Œé€‰æ‹©æ£€æŸ¥é¡¹æ•°é‡æœ€å¤šçš„ç»“æœ")
        logger.info("")
        
        # ç¡®å®šè¦æå–çš„åŸºå‡†æ–‡æ¡£åˆ—è¡¨
        baseline_docs = []
        if baseline_dir:
            # å¦‚æœæŒ‡å®šäº†åŸºå‡†ç›®å½•ï¼Œæå–ç›®å½•ä¸­çš„æ‰€æœ‰ .md æ–‡ä»¶ï¼ˆé€’å½’ï¼‰
            md_files = sorted(baseline_dir.rglob("*.md")) + sorted(baseline_dir.rglob("*.markdown"))
            if not md_files:
                logger.error(f"åŸºå‡†æ–‡æ¡£æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ° .md æ–‡ä»¶: {args.baseline_dir}")
                sys.exit(1)
            baseline_docs.extend(md_files)
            logger.info(f"æ‰¾åˆ° {len(baseline_docs)} ä¸ªåŸºå‡†æ–‡æ¡£")
        elif baseline_path:
            # å¦‚æœåªæŒ‡å®šäº†å•ä¸ªåŸºå‡†æ–‡æ¡£ï¼Œåªå¤„ç†è¯¥æ–‡æ¡£
            baseline_docs.append(baseline_path)
        
        # æ£€æŸ¥ç¼“å­˜å¹¶è¿‡æ»¤éœ€è¦å¤„ç†çš„æ–‡æ¡£
        from src.document_parser import DocumentParser
        parser = DocumentParser()
        docs_to_process = []
        skipped_count = 0
        
        if not args.force_extract:
            logger.info("æ£€æŸ¥ç¼“å­˜çŠ¶æ€...")
            for doc_path in baseline_docs:
                try:
                    # è¯»å–æ–‡æ¡£å†…å®¹ä»¥è®¡ç®—hash
                    content = parser.read_markdown(doc_path)
                    content_hash = extractor._get_content_hash(content)
                    
                    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
                    if extractor.has_cache(doc_path, content_hash):
                        logger.info(f"âŠ˜ {doc_path.name} - ç¼“å­˜å·²å­˜åœ¨ï¼Œè·³è¿‡")
                        skipped_count += 1
                    else:
                        docs_to_process.append(doc_path)
                except Exception as e:
                    logger.warning(f"âš  {doc_path.name} - æ£€æŸ¥ç¼“å­˜æ—¶å‡ºé”™: {e}ï¼Œå°†å°è¯•æå–")
                    docs_to_process.append(doc_path)
        else:
            docs_to_process = baseline_docs
        
        if skipped_count > 0:
            logger.info(f"å·²è·³è¿‡ {skipped_count} ä¸ªå·²æœ‰ç¼“å­˜çš„æ–‡æ¡£")
        if docs_to_process:
            logger.info(f"éœ€è¦å¤„ç† {len(docs_to_process)} ä¸ªæ–‡æ¡£")
        logger.info("")
        
        if not docs_to_process:
            logger.info("=" * 60)
            logger.info("æ‰€æœ‰æ–‡æ¡£çš„ç¼“å­˜å·²å­˜åœ¨ï¼Œæ— éœ€å¤„ç†ï¼")
            logger.info("=" * 60)
            sys.exit(0)
        
        # ç¡®å®šæ˜¯å¦å¹¶è¡Œæ‰§è¡Œ
        parallel_extract = len(docs_to_process) > 1
        max_workers = args.max_workers
        
        if parallel_extract:
            if max_workers is None:
                max_workers = min(len(docs_to_process), 10)  # æœ€å¤š10ä¸ªå¹¶è¡Œ
            logger.info(f"â„¹ å¹¶è¡Œæ‰§è¡Œæ¨¡å¼ï¼šæœ€å¤§å·¥ä½œçº¿ç¨‹æ•° = {max_workers}")
            logger.info("")
        
        # æå–è¦ç‚¹ç¼“å­˜çš„å‡½æ•°
        def extract_document(doc_path: Path) -> tuple[Path, bool, int | None]:
            """æå–å•ä¸ªæ–‡æ¡£çš„è¦ç‚¹ç¼“å­˜"""
            try:
                checkpoints = extractor.extract_points(
                    doc_path,
                    force_extract=args.force_extract,
                    extract_runs=args.extract_runs,
                )
                return (doc_path, True, len(checkpoints))
            except Exception as e:
                logger.error(f"âœ— {doc_path.name} - æå–è¦ç‚¹å¤±è´¥: {e}")
                logger.debug(f"æå–è¦ç‚¹å¤±è´¥è¯¦æƒ…:", exc_info=True)
                return (doc_path, False, None)
        
        # æå–è¦ç‚¹ç¼“å­˜
        success_count = 0
        if parallel_extract:
            # å¹¶è¡Œæ‰§è¡Œ
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {
                    executor.submit(extract_document, doc_path): doc_path
                    for doc_path in docs_to_process
                }
                
                completed = 0
                for future in as_completed(futures):
                    completed += 1
                    doc_path, success, checkpoint_count = future.result()
                    if success:
                        success_count += 1
                        logger.info(
                            f"[{completed}/{len(docs_to_process)}] âœ“ {doc_path.name} - "
                            f"æ£€æŸ¥é¡¹æ¸…å•ï¼šå…± {checkpoint_count} ä¸ªæ£€æŸ¥é¡¹"
                        )
                    else:
                        logger.info(f"[{completed}/{len(docs_to_process)}] âœ— {doc_path.name} - æå–å¤±è´¥")
        else:
            # ä¸²è¡Œæ‰§è¡Œ
            for doc_path in docs_to_process:
                logger.info(f"æ­£åœ¨ä»åŸºå‡†æ–‡æ¡£æå–è¦ç‚¹æ¸…å•: {doc_path.name}")
                logger.info("-" * 60)
                
                doc_path, success, checkpoint_count = extract_document(doc_path)
                if success:
                    success_count += 1
                    logger.info(f"âœ“ {doc_path.name} - æ£€æŸ¥é¡¹æ¸…å•ï¼šå…± {checkpoint_count} ä¸ªæ£€æŸ¥é¡¹")
                logger.info("")
        
        logger.info("=" * 60)
        logger.info(f"è¦ç‚¹ç¼“å­˜æ„å»ºå®Œæˆï¼")
        logger.info(f"  æˆåŠŸ: {success_count}/{len(docs_to_process)}")
        if skipped_count > 0:
            logger.info(f"  è·³è¿‡: {skipped_count} (å·²æœ‰ç¼“å­˜)")
        logger.info(f"  æ€»è®¡: {len(baseline_docs)}")
        logger.info("=" * 60)
        sys.exit(0)

    # ç¡®å®šå¾…è¯„ä¼°æ–‡æ¡£åˆ—è¡¨
    target_paths = []
    if args.target:
        target_paths.append(Path(args.target))
    if args.targets:
        target_paths.extend([Path(t) for t in args.targets])
    if args.target_dir:
        target_dir = Path(args.target_dir)
        if not target_dir.exists():
            logger.error(f"å¾…è¯„ä¼°æ–‡æ¡£æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {args.target_dir}")
            sys.exit(1)
        if not target_dir.is_dir():
            logger.error(f"å¾…è¯„ä¼°æ–‡æ¡£è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {args.target_dir}")
            sys.exit(1)
        # æ‰«ææ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ .md æ–‡ä»¶ï¼ˆé€’å½’ï¼‰
        md_files = sorted(target_dir.rglob("*.md")) + sorted(target_dir.rglob("*.markdown"))
        if not md_files:
            logger.error(f"å¾…è¯„ä¼°æ–‡æ¡£æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ° .md æ–‡ä»¶: {args.target_dir}")
            sys.exit(1)
        
        # å¦‚æœæŒ‡å®šäº† --force-re-evalï¼Œåªè¯„ä¼°æŒ‡å®šçš„æ–‡ä»¶
        if args.force_re_eval:
            # å‡†å¤‡å¼ºåˆ¶é‡æ–°è¯„ä¼°çš„æ–‡æ¡£åç§°é›†åˆï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
            force_re_eval_set = set()
            for item in args.force_re_eval:
                # æ”¯æŒæ–‡æ¡£åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰æˆ–å®Œæ•´è·¯å¾„
                item_path = Path(item)
                if item_path.is_absolute() or item_path.exists():
                    # æ˜¯å®Œæ•´è·¯å¾„
                    force_re_eval_set.add(item_path.stem)
                else:
                    # æ˜¯æ–‡æ¡£åç§°ï¼ˆå»æ‰æ‰©å±•åä»¥åŒ¹é… doc_name æ ¼å¼ï¼‰
                    force_re_eval_set.add(item_path.stem)
            
            # åªä¿ç•™åœ¨ force_re_eval_set ä¸­çš„æ–‡ä»¶
            filtered_files = [f for f in md_files if f.stem in force_re_eval_set]
            if not filtered_files:
                logger.warning(f"åœ¨ç›®å½•ä¸­æœªæ‰¾åˆ° --force-re-eval æŒ‡å®šçš„æ–‡ä»¶")
                logger.info(f"  æŒ‡å®šçš„æ–‡ä»¶: {args.force_re_eval}")
                logger.info(f"  ç›®å½•ä¸­çš„æ–‡ä»¶æ•°é‡: {len(md_files)}")
            else:
                logger.info(f"ä»å¾…è¯„ä¼°æ–‡æ¡£æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ° {len(md_files)} ä¸ªæ–‡æ¡£ï¼Œå°†åªè¯„ä¼° {len(filtered_files)} ä¸ªæŒ‡å®šæ–‡ä»¶")
                md_files = filtered_files
        else:
            logger.info(f"ä»å¾…è¯„ä¼°æ–‡æ¡£æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ° {len(md_files)} ä¸ªæ–‡æ¡£")
        
        target_paths.extend(md_files)
    
    # å¤„ç†srs_collectionç›®å½•ï¼ˆæŒ‰é˜¶æ®µåˆ†ç»„è¯„ä¼°ï¼‰
    stage_evaluations = {}  # å­˜å‚¨æ¯ä¸ªé˜¶æ®µçš„è¯„ä¼°ç»“æœ
    if args.srs_collection_dir:
        srs_collection_dir = Path(args.srs_collection_dir)
        if not srs_collection_dir.exists():
            logger.error(f"srs_collectionç›®å½•ä¸å­˜åœ¨: {args.srs_collection_dir}")
            sys.exit(1)
        if not srs_collection_dir.is_dir():
            logger.error(f"srs_collectionè·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {args.srs_collection_dir}")
            sys.exit(1)
        
        # æ£€æµ‹å„ä¸ªé˜¶æ®µ
        stages = detect_stages(srs_collection_dir)
        if not stages:
            logger.error(f"åœ¨srs_collectionç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•é˜¶æ®µç›®å½•: {args.srs_collection_dir}")
            sys.exit(1)
        
        # å¯¹é˜¶æ®µè¿›è¡Œæ’åº
        sorted_stage_names = sort_stage_names(list(stages.keys()))
        logger.info(f"æ£€æµ‹åˆ° {len(sorted_stage_names)} ä¸ªé˜¶æ®µ: {', '.join(sorted_stage_names)}")
        
        # ä¸ºæ¯ä¸ªé˜¶æ®µè¿›è¡Œè¯„ä¼°
        for stage_name in sorted_stage_names:
            stage_docs = stages[stage_name]
            logger.info("")
            logger.info("=" * 60)
            logger.info(f"å¼€å§‹è¯„ä¼°é˜¶æ®µ: {stage_name} ({len(stage_docs)} ä¸ªæ–‡æ¡£)")
            logger.info("=" * 60)
            
            # ä¸ºæ¯ä¸ªé˜¶æ®µåˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•
            stage_output_dir = Path(args.output_dir) / stage_name
            stage_output_dir.mkdir(parents=True, exist_ok=True)
            
            # è¯„ä¼°è¯¥é˜¶æ®µçš„æ‰€æœ‰æ–‡æ¡£
            stage_eval_results = evaluate_stage(
                stage_docs,
                baseline_path,
                baseline_dir,
                stage_output_dir,
                config,
                args,
                judges,
                logger
            )
            
            stage_evaluations[stage_name] = {
                "evaluations": stage_eval_results,
                "output_dir": stage_output_dir
            }
            
            logger.info(f"é˜¶æ®µ {stage_name} è¯„ä¼°å®Œæˆï¼Œå…± {len(stage_eval_results)} ä¸ªæ–‡æ¡£")
        
        # ç”Ÿæˆè·¨é˜¶æ®µå¯¹æ¯”æŠ¥å‘Š
        if len(stage_evaluations) > 1:
            logger.info("")
            logger.info("=" * 60)
            logger.info("ç”Ÿæˆè·¨é˜¶æ®µå¯¹æ¯”æŠ¥å‘Š...")
            logger.info("=" * 60)
            
            cross_stage_report_path = Path(args.output_dir) / "cross_stage_comparison.md"
            formatter = OutputFormatter()
            cross_stage_report = formatter.generate_cross_stage_comparison_report(
                stage_evaluations,
                baseline_dir=baseline_dir,
                output_dir=Path(args.output_dir)
            )
            
            with open(cross_stage_report_path, "w", encoding="utf-8") as f:
                f.write(cross_stage_report)
            
            logger.info(f"âœ“ è·¨é˜¶æ®µå¯¹æ¯”æŠ¥å‘Š: {cross_stage_report_path}")
        
        # é€€å‡ºï¼Œå› ä¸ºå·²ç»å®Œæˆäº†æ‰€æœ‰è¯„ä¼°
        logger.info("")
        logger.info("æ‰€æœ‰é˜¶æ®µè¯„ä¼°å®Œæˆï¼")
        sys.exit(0)

    # éªŒè¯å¾…è¯„ä¼°æ–‡æ¡£
    for target_path in target_paths:
        if not target_path.exists():
            logger.error(f"å¾…è¯„ä¼°æ–‡æ¡£ä¸å­˜åœ¨: {target_path}")
            sys.exit(1)
        if not target_path.is_file():
            logger.error(f"å¾…è¯„ä¼°æ–‡æ¡£è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {target_path}")
            sys.exit(1)

    # å¦‚æœä¸æ˜¯åŒ¹é…æ¨¡å¼ï¼Œåˆ™ä»å•ä¸ªåŸºå‡†æ–‡æ¡£æå–è¦ç‚¹æ¸…å•
    if not use_matching_mode:
        logger.info(f"æ­£åœ¨ä»åŸºå‡†æ–‡æ¡£æå–è¦ç‚¹æ¸…å•: {baseline_path}")
        logger.info("-" * 60)

        # æå–è¦ç‚¹æ¸…å•
        try:
            extractor = PointExtractor(config, prompt_version=config.prompt_version)
            logger.info(f"ä½¿ç”¨æ¨¡å‹: {config.openai.model}")
            logger.info(f"APIåœ°å€: {config.openai.base_url}")
            if args.prompt_version:
                logger.info(f"æç¤ºè¯ç‰ˆæœ¬: {config.prompt_version}")
            
            if args.force_extract:
                logger.info("âš  å¼ºåˆ¶é‡æ–°æå–æ¨¡å¼ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰")
            else:
                logger.info("â„¹ ä½¿ç”¨ç¼“å­˜æœºåˆ¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰")
            
            if args.extract_runs > 1:
                logger.info(f"â„¹ å¤šæ¬¡æå–æ¨¡å¼ï¼šå°†æ‰§è¡Œ {args.extract_runs} æ¬¡æå–ï¼Œé€‰æ‹©æ£€æŸ¥é¡¹æ•°é‡æœ€å¤šçš„ç»“æœ")
            
            checkpoints = extractor.extract_points(
                baseline_path,
                force_extract=args.force_extract,
                extract_runs=args.extract_runs,
            )
            
            logger.info(f"âœ“ æ£€æŸ¥é¡¹æ¸…å•ï¼šå…± {len(checkpoints)} ä¸ªæ£€æŸ¥é¡¹")
            logger.info("")
        except Exception as e:
            logger.error(f"æå–è¦ç‚¹å¤±è´¥: {e}")
            logger.debug(f"è°ƒè¯•ä¿¡æ¯:", exc_info=True)
            logger.debug(f"  - åŸºå‡†æ–‡æ¡£: {baseline_path}")
            logger.debug(f"  - æ–‡æ¡£æ˜¯å¦å­˜åœ¨: {baseline_path.exists()}")
            if baseline_path.exists():
                try:
                    from src.document_parser import DocumentParser
                    parser = DocumentParser()
                    content = parser.read_markdown(baseline_path)
                    logger.debug(f"  - æ–‡æ¡£å¤§å°: {len(content)} å­—ç¬¦")
                except Exception as e2:
                    logger.debug(f"  - è¯»å–æ–‡æ¡£å¤±è´¥: {e2}")
            logger.debug(f"  - æ¨¡å‹: {config.openai.model}")
            logger.debug(f"  - APIåœ°å€: {config.openai.base_url}")
            sys.exit(1)
    else:
        # åŒ¹é…æ¨¡å¼ï¼šä¸åœ¨è¿™é‡Œæå–è¦ç‚¹æ¸…å•ï¼Œè€Œæ˜¯ä¸ºæ¯ä¸ªç›®æ ‡æ–‡æ¡£å•ç‹¬æå–
        checkpoints = None
        extractor = PointExtractor(config, prompt_version=config.prompt_version)
        logger.info(f"ä½¿ç”¨æ¨¡å‹: {config.openai.model}")
        logger.info(f"APIåœ°å€: {config.openai.base_url}")
        if args.prompt_version:
            logger.info(f"æç¤ºè¯ç‰ˆæœ¬: {config.prompt_version}")
        if args.force_extract:
            logger.info("âš  å¼ºåˆ¶é‡æ–°æå–æ¨¡å¼ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰")
        else:
            logger.info("â„¹ ä½¿ç”¨ç¼“å­˜æœºåˆ¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰")
        if args.extract_runs > 1:
            logger.info(f"â„¹ å¤šæ¬¡æå–æ¨¡å¼ï¼šå°†æ‰§è¡Œ {args.extract_runs} æ¬¡æå–ï¼Œé€‰æ‹©æ£€æŸ¥é¡¹æ•°é‡æœ€å¤šçš„ç»“æœ")
        logger.info("")

    # è¾“å‡ºç›®å½•ï¼ˆæå‰åˆ›å»ºï¼Œç”¨äºæ£€æŸ¥å·²å­˜åœ¨çš„æ–‡ä»¶ï¼‰
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    formatter = OutputFormatter()

    # å¦‚æœä½¿ç”¨ --skip-existingï¼Œæ£€æŸ¥å¹¶åŠ è½½å·²å­˜åœ¨çš„è¯„ä¼°ç»“æœ
    evaluations = []
    if args.skip_existing:
        logger.info("æ£€æŸ¥å·²å­˜åœ¨çš„è¯„ä¼°ç»“æœ...")
        logger.info("-" * 60)
        
        existing_evaluations = []
        new_target_paths = []
        
        # å‡†å¤‡å¼ºåˆ¶é‡æ–°è¯„ä¼°çš„æ–‡æ¡£åç§°é›†åˆï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
        force_re_eval_set = set()
        if args.force_re_eval:
            for item in args.force_re_eval:
                # æ”¯æŒæ–‡æ¡£åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰æˆ–å®Œæ•´è·¯å¾„
                item_path = Path(item)
                if item_path.is_absolute() or item_path.exists():
                    # æ˜¯å®Œæ•´è·¯å¾„
                    force_re_eval_set.add(item_path.stem)
                else:
                    # æ˜¯æ–‡æ¡£åç§°ï¼ˆå»æ‰æ‰©å±•åä»¥åŒ¹é… doc_name æ ¼å¼ï¼‰
                    force_re_eval_set.add(item_path.stem)
        
        for target_path in target_paths:
            doc_name = Path(target_path).stem
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶é‡æ–°è¯„ä¼°
            if args.force_re_eval and doc_name in force_re_eval_set:
                logger.info(f"ğŸ”„ {doc_name} - å¼ºåˆ¶é‡æ–°è¯„ä¼°ï¼ˆå¿½ç•¥å·²å­˜åœ¨çš„ç»“æœï¼‰")
                new_target_paths.append(target_path)
                continue
            
            json_path = output_dir / f"{doc_name}_evaluation.json"
            md_path = output_dir / f"{doc_name}_evaluation.md"
            
            # ä¼˜å…ˆæ£€æŸ¥ JSON æ–‡ä»¶ï¼ˆåŒ…å«å®Œæ•´è¯„ä¼°æ•°æ®ï¼‰
            if json_path.exists():
                # å°è¯•åŠ è½½å·²å­˜åœ¨çš„è¯„ä¼°ç»“æœ
                existing_eval = formatter.load_json(json_path)
                if existing_eval:
                    existing_evaluations.append(existing_eval)
                    logger.info(f"âŠ˜ {doc_name} - å·²å­˜åœ¨ï¼ˆJSONï¼‰ï¼Œè·³è¿‡è¯„ä¼°")
                else:
                    # åŠ è½½å¤±è´¥ï¼Œéœ€è¦é‡æ–°è¯„ä¼°
                    new_target_paths.append(target_path)
            # å¦‚æœ JSON ä¸å­˜åœ¨ï¼Œæ£€æŸ¥ Markdown æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆä½œä¸ºå·²å­˜åœ¨çš„æ ‡å¿—ï¼‰
            elif md_path.exists():
                # Markdown æ–‡ä»¶å­˜åœ¨ä½† JSON ä¸å­˜åœ¨ï¼Œå°è¯•ä» Markdown è§£æè¯„ä¼°ç»“æœ
                existing_eval = formatter.load_from_markdown(md_path)
                if existing_eval:
                    existing_evaluations.append(existing_eval)
                    # ä¿å­˜ä¸º JSON æ–‡ä»¶ï¼Œä»¥ä¾¿ä¸‹æ¬¡ç›´æ¥åŠ è½½
                    formatter.save_json(existing_eval, json_path)
                    logger.info(f"âŠ˜ {doc_name} - å·²å­˜åœ¨ï¼ˆMarkdownï¼‰ï¼Œå·²ä»Markdownè§£æå¹¶ä¿å­˜ä¸ºJSONï¼Œè·³è¿‡è¯„ä¼°")
                else:
                    # è§£æå¤±è´¥ï¼Œéœ€è¦é‡æ–°è¯„ä¼°
                    logger.warning(f"âš  {doc_name} - Markdownå­˜åœ¨ä½†è§£æå¤±è´¥ï¼Œå°†é‡æ–°è¯„ä¼°")
                    new_target_paths.append(target_path)
            else:
                # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦è¯„ä¼°
                new_target_paths.append(target_path)
        
        if existing_evaluations:
            logger.info(f"å·²è·³è¿‡ {len(existing_evaluations)} ä¸ªå·²å­˜åœ¨çš„è¯„ä¼°ç»“æœï¼ˆå·²åŠ è½½ï¼‰")
        if new_target_paths:
            logger.info(f"éœ€è¦è¯„ä¼° {len(new_target_paths)} ä¸ªæ–°æ–‡æ¡£")
        logger.info("")
        
        # å°†å·²å­˜åœ¨çš„è¯„ä¼°ç»“æœæ·»åŠ åˆ° evaluations åˆ—è¡¨
        evaluations.extend(existing_evaluations)
        
        # æ›´æ–° target_paths ä¸ºéœ€è¦è¯„ä¼°çš„æ–‡æ¡£
        target_paths = new_target_paths

    # è¯„ä¼°æ–‡æ¡£ï¼ˆæ”¯æŒå¹¶è¡Œæ‰§è¡Œï¼‰
    evaluator = Evaluator(config, prompt_version=config.prompt_version)

    # ç¡®å®šæ˜¯å¦å¹¶è¡Œæ‰§è¡Œï¼ˆä»…æ ¹æ®æ–‡æ¡£æ•°é‡ï¼Œjudges ç°åœ¨æ˜¯ä¸²è¡Œçš„ï¼‰
    parallel_eval = len(target_paths) > 1
    max_workers = args.max_workers
    
    if parallel_eval and target_paths:
        if max_workers is None:
            # è‡ªåŠ¨è®¡ç®—ï¼šå¦‚æœæ˜¯æ‰¹é‡è¯„ä¼°ï¼Œæ¯ä¸ªæ–‡æ¡£å¹¶è¡Œ
            max_workers = min(len(target_paths), 10)  # æœ€å¤š10ä¸ªå¹¶è¡Œ
        logger.info(f"â„¹ å¹¶è¡Œæ‰§è¡Œæ¨¡å¼ï¼šæœ€å¤§å·¥ä½œçº¿ç¨‹æ•° = {max_workers}")
        logger.info("")

    def evaluate_document(target_path: Path) -> tuple[Path, DocumentEvaluation | None]:
        """è¯„ä¼°å•ä¸ªæ–‡æ¡£çš„å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œæ‰§è¡Œ"""
        try:
            # å¦‚æœæ˜¯åŒ¹é…æ¨¡å¼ï¼Œä¸ºæ¯ä¸ªç›®æ ‡æ–‡æ¡£æ‰¾åˆ°å¯¹åº”çš„åŸºå‡†æ–‡æ¡£
            doc_baseline_path = baseline_path
            doc_checkpoints = checkpoints
            
            if use_matching_mode:
                # æŸ¥æ‰¾åŒ¹é…çš„åŸºå‡†æ–‡æ¡£
                matched_baseline = find_matching_baseline(target_path, baseline_dir)
                if matched_baseline is None:
                    logger.warning(f"æœªæ‰¾åˆ° {target_path.name} çš„åŒ¹é…åŸºå‡†æ–‡æ¡£ï¼Œè·³è¿‡è¯„ä¼°")
                    return (target_path, None)
                doc_baseline_path = matched_baseline
                
                # ä»å¯¹åº”çš„åŸºå‡†æ–‡æ¡£æå–æ£€æŸ¥é¡¹æ¸…å•
                try:
                    doc_checkpoints = extractor.extract_points(
                        doc_baseline_path,
                        force_extract=args.force_extract,
                        extract_runs=args.extract_runs,
                    )
                except Exception as e:
                    logger.error(f"ä»åŸºå‡†æ–‡æ¡£ {doc_baseline_path.name} æå–è¦ç‚¹å¤±è´¥: {e}")
                    logger.debug(f"æå–è¦ç‚¹å¤±è´¥è¯¦æƒ…:", exc_info=True)
                    return (target_path, None)
            
            if judges > 1:
                evaluation = evaluator.evaluate_multiple_runs(
                    doc_checkpoints, target_path, runs=judges, baseline_document_path=doc_baseline_path
                )
            else:
                # è®°å½•å¼€å§‹æ—¶é—´
                start_time = time.time()
                evaluation_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                
                evaluation = evaluator.evaluate_single_run(doc_checkpoints, target_path)
                # ä¸ºå•æ¬¡è¯„ä¼°ä¹Ÿæ·»åŠ å…ƒä¿¡æ¯
                evaluation.model_name = config.openai.model
                evaluation.baseline_document = str(doc_baseline_path)
                evaluation.evaluation_time = evaluation_time
                evaluation.evaluation_duration = time.time() - start_time
            return (target_path, evaluation)
        except Exception as e:
            logger.error(f"è¯„ä¼°æ–‡æ¡£ {target_path} å¤±è´¥: {e}")
            logger.debug(f"è¯„ä¼°æ–‡æ¡£ {target_path} å¤±è´¥è¯¦æƒ…:", exc_info=True)
            return (target_path, None)

    if parallel_eval and len(target_paths) > 1:
        # æ‰¹é‡è¯„ä¼°å¤šä¸ªæ–‡æ¡£ï¼Œå¹¶è¡Œæ‰§è¡Œ
        logger.info(f"æ­£åœ¨å¹¶è¡Œè¯„ä¼° {len(target_paths)} ä¸ªæ–‡æ¡£...")
        logger.info("-" * 60)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(evaluate_document, target_path): target_path
                for target_path in target_paths
            }
            
            completed = 0
            for future in as_completed(futures):
                completed += 1
                target_path, evaluation = future.result()
                if evaluation is not None:
                    evaluations.append(evaluation)
                    baseline_info = ""
                    if use_matching_mode and evaluation.baseline_document:
                        baseline_name = Path(evaluation.baseline_document).name
                        baseline_info = f" (åŸºå‡†: {baseline_name})"
                    weighted_score = OutputFormatter._calculate_weighted_score(evaluation)
                    logger.info(
                        f"[{completed}/{len(target_paths)}] âœ“ {target_path.name}{baseline_info} - "
                        f"åŠ æƒå¾—åˆ†: {weighted_score:.2f}"
                    )
        logger.info("")
    else:
        # ä¸²è¡Œæ‰§è¡Œï¼ˆå•ä¸ªæ–‡æ¡£æˆ–ä¸éœ€è¦å¹¶è¡Œï¼‰
        for target_path in target_paths:
            logger.info(f"æ­£åœ¨è¯„ä¼°æ–‡æ¡£: {target_path}")
            if judges > 1:
                logger.info(f"è¯„å§”æ•°é‡: {judges}")
            
            # å¦‚æœæ˜¯åŒ¹é…æ¨¡å¼ï¼Œä¸ºæ¯ä¸ªç›®æ ‡æ–‡æ¡£æ‰¾åˆ°å¯¹åº”çš„åŸºå‡†æ–‡æ¡£
            doc_baseline_path = baseline_path
            doc_checkpoints = checkpoints
            
            if use_matching_mode:
                # æŸ¥æ‰¾åŒ¹é…çš„åŸºå‡†æ–‡æ¡£
                matched_baseline = find_matching_baseline(target_path, baseline_dir)
                if matched_baseline is None:
                    logger.warning(f"æœªæ‰¾åˆ° {target_path.name} çš„åŒ¹é…åŸºå‡†æ–‡æ¡£ï¼Œè·³è¿‡è¯„ä¼°")
                    continue
                doc_baseline_path = matched_baseline
                logger.info(f"åŒ¹é…çš„åŸºå‡†æ–‡æ¡£: {doc_baseline_path.name}")
                
                # ä»å¯¹åº”çš„åŸºå‡†æ–‡æ¡£æå–æ£€æŸ¥é¡¹æ¸…å•
                try:
                    logger.info(f"æ­£åœ¨ä»åŸºå‡†æ–‡æ¡£æå–è¦ç‚¹æ¸…å•: {doc_baseline_path.name}")
                    doc_checkpoints = extractor.extract_points(
                        doc_baseline_path,
                        force_extract=args.force_extract,
                        extract_runs=args.extract_runs,
                    )
                    logger.info(f"âœ“ æ£€æŸ¥é¡¹æ¸…å•ï¼šå…± {len(doc_checkpoints)} ä¸ªæ£€æŸ¥é¡¹")
                except Exception as e:
                    logger.error(f"ä»åŸºå‡†æ–‡æ¡£ {doc_baseline_path.name} æå–è¦ç‚¹å¤±è´¥: {e}")
                    logger.debug(f"æå–è¦ç‚¹å¤±è´¥è¯¦æƒ…:", exc_info=True)
                    continue
            
            try:
                if judges > 1:
                    evaluation = evaluator.evaluate_multiple_runs(
                        doc_checkpoints, target_path, runs=judges, baseline_document_path=doc_baseline_path
                    )
                else:
                    # è®°å½•å¼€å§‹æ—¶é—´
                    start_time = time.time()
                    evaluation_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    
                    evaluation = evaluator.evaluate_single_run(doc_checkpoints, target_path)
                    # ä¸ºå•æ¬¡è¯„ä¼°ä¹Ÿæ·»åŠ å…ƒä¿¡æ¯
                    evaluation.model_name = config.openai.model
                    evaluation.baseline_document = str(doc_baseline_path)
                    evaluation.evaluation_time = evaluation_time
                    evaluation.evaluation_duration = time.time() - start_time

                evaluations.append(evaluation)
                weighted_score = OutputFormatter._calculate_weighted_score(evaluation)
                logger.info(f"âœ“ è¯„ä¼°å®Œæˆ - åŠ æƒå¾—åˆ†: {weighted_score:.2f}")
                logger.info("")
            except Exception as e:
                logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
                logger.debug(f"è¯„ä¼°å¤±è´¥è¯¦æƒ…:", exc_info=True)
                continue

    if not evaluations:
        logger.error("æ²¡æœ‰æˆåŠŸè¯„ä¼°ä»»ä½•æ–‡æ¡£ï¼Œä¹Ÿæ²¡æœ‰å·²å­˜åœ¨çš„è¯„ä¼°ç»“æœ")
        sys.exit(1)

    # ä¿å­˜æ–°è¯„ä¼°çš„ç»“æœ
    if target_paths:
        logger.info("æ­£åœ¨ä¿å­˜è¯„ä¼°ç»“æœ...")
        logger.info("-" * 60)
        
        # åªä¿å­˜æ–°è¯„ä¼°çš„ç»“æœï¼ˆtarget_paths ä¸­çš„æ–‡æ¡£ï¼‰
        new_evaluations = [e for e in evaluations if Path(e.target_document) in target_paths]
        for evaluation in new_evaluations:
            doc_name = Path(evaluation.target_document).stem

            # æ€»æ˜¯ä¿å­˜ JSON æ–‡ä»¶ï¼ˆç”¨äº --skip-existing åŠŸèƒ½ï¼‰
            json_path = output_dir / f"{doc_name}_evaluation.json"
            formatter.save_json(evaluation, json_path)
            logger.info(f"âœ“ JSON: {json_path}")

            if args.output in ["markdown", "all"]:
                md_path = output_dir / f"{doc_name}_evaluation.md"
                formatter.save_markdown(evaluation, md_path)
                logger.info(f"âœ“ Markdown: {md_path}")
            
            # å§‹ç»ˆä¿å­˜ TSV æ–‡ä»¶ï¼ˆåŒ…å«æ‰€æœ‰è¯„å§”çš„è¯¦ç»†ç»“æœï¼‰
            tsv_path = output_dir / f"{doc_name}_evaluation.tsv"
            formatter.save_tsv(evaluation, tsv_path)
            logger.info(f"âœ“ TSV: {tsv_path}")
        logger.info("")

    if args.output in ["csv", "all"]:
        csv_path = output_dir / "evaluations_summary.csv"
        formatter.to_csv(evaluations, csv_path)
        logger.info(f"âœ“ CSV: {csv_path}")

    # å¦‚æœæœ‰å¤šä¸ªè¯„ä¼°ç»“æœï¼Œç”Ÿæˆèšåˆç»Ÿè®¡æŠ¥å‘Š
    # åŸºäºè¾“å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æœ€æ–°è¯„ä¼°ç»“æœé‡æ–°ç”ŸæˆèšåˆæŠ¥å‘Š
    if len(evaluations) > 1 or output_dir.exists():
        logger.info("")
        logger.info("æ­£åœ¨ç”Ÿæˆèšåˆç»Ÿè®¡æŠ¥å‘Š...")
        summary_path = output_dir / "summary_report.md"
        total_time = time.time() - total_start_time
        
        # ä»è¾“å‡ºç›®å½•ä¸­åŠ è½½æ‰€æœ‰è¯„ä¼°ç»“æœï¼ˆåŸºäºæœ€æ–°æ•°æ®ï¼‰
        all_evaluations = []
        json_files = sorted(output_dir.glob("*_evaluation.json"))
        
        if json_files:
            logger.info(f"ä»è¾“å‡ºç›®å½•åŠ è½½ {len(json_files)} ä¸ªè¯„ä¼°ç»“æœ...")
            for json_file in json_files:
                eval_result = formatter.load_json(json_file)
                if eval_result:
                    all_evaluations.append(eval_result)
            
            if all_evaluations:
                logger.info(f"æˆåŠŸåŠ è½½ {len(all_evaluations)} ä¸ªè¯„ä¼°ç»“æœ")
            else:
                logger.warning("æœªèƒ½åŠ è½½ä»»ä½•è¯„ä¼°ç»“æœï¼Œä½¿ç”¨æœ¬æ¬¡è¯„ä¼°çš„ç»“æœ")
                all_evaluations = evaluations
        else:
            # å¦‚æœæ²¡æœ‰JSONæ–‡ä»¶ï¼Œä½¿ç”¨æœ¬æ¬¡è¯„ä¼°çš„ç»“æœ
            all_evaluations = evaluations
        
        # å¦‚æœä½¿ç”¨äº† --force-re-evalï¼ŒåªåŒ…å«æŒ‡å®šçš„æ–‡ä»¶
        report_evaluations = all_evaluations
        if args.force_re_eval:
            # å‡†å¤‡å¼ºåˆ¶é‡æ–°è¯„ä¼°çš„æ–‡æ¡£åç§°é›†åˆ
            force_re_eval_set = set()
            for item in args.force_re_eval:
                item_path = Path(item)
                if item_path.is_absolute() or item_path.exists():
                    force_re_eval_set.add(item_path.stem)
                else:
                    force_re_eval_set.add(item_path.stem)
            
            # åªä¿ç•™æŒ‡å®šçš„æ–‡ä»¶ï¼ˆåœ¨ force_re_eval_set ä¸­çš„æ–‡ä»¶ï¼‰
            report_evaluations = [
                eval for eval in all_evaluations 
                if Path(eval.target_document).stem in force_re_eval_set
            ]
            
            if len(report_evaluations) != len(all_evaluations):
                logger.info(f"èšåˆæŠ¥å‘Šå°†åªåŒ…å«æŒ‡å®šçš„ {len(report_evaluations)} ä¸ªæ–‡ä»¶ï¼ˆè¾“å‡ºç›®å½•ä¸­å…± {len(all_evaluations)} ä¸ªè¯„ä¼°ç»“æœï¼‰")
        
        if len(report_evaluations) > 1:
            # ç¡®å®štarget_dirå’Œbaseline_dir
            target_dir_path = None
            if args.target_dir:
                target_dir_path = Path(args.target_dir)
            baseline_dir_path = None
            if args.baseline_dir:
                baseline_dir_path = Path(args.baseline_dir)
            formatter.save_summary_report(
                report_evaluations, 
                summary_path, 
                baseline_path,
                target_dir=target_dir_path,
                baseline_dir=baseline_dir_path,
                output_dir=output_dir,
                judges=judges,
                total_time=total_time,
            )
            logger.info(f"âœ“ èšåˆç»Ÿè®¡æŠ¥å‘Š: {summary_path} (åŸºäº {len(report_evaluations)} ä¸ªè¯„ä¼°ç»“æœ)")
        elif len(report_evaluations) == 1:
            logger.info("åªæœ‰1ä¸ªè¯„ä¼°ç»“æœï¼Œè·³è¿‡èšåˆæŠ¥å‘Šç”Ÿæˆ")
        else:
            logger.warning("æ²¡æœ‰è¯„ä¼°ç»“æœï¼Œè·³è¿‡èšåˆæŠ¥å‘Šç”Ÿæˆ")

    logger.info("")
    logger.info("è¯„ä¼°å®Œæˆï¼")

    # æ‰“å°ç®€è¦æ€»ç»“
    logger.info("\nè¯„ä¼°æ€»ç»“:")
    logger.info("-" * 60)
    for evaluation in evaluations:
        doc_name = Path(evaluation.target_document).name
        weighted_score = OutputFormatter._calculate_weighted_score(evaluation)
        logger.info(f"{doc_name}: åŠ æƒå¾—åˆ†={weighted_score:.2f}")
    
    # å¦‚æœæœ‰å¤šä¸ªè¯„ä¼°ç»“æœï¼Œæ‰“å°èšåˆç»Ÿè®¡
    if len(evaluations) > 1:
        logger.info("")
        logger.info("èšåˆç»Ÿè®¡:")
        logger.info("-" * 60)
        weighted_scores = [
            OutputFormatter._calculate_weighted_score(evaluation)
            for evaluation in evaluations
        ]
        logger.info(
            f"åŠ æƒå¾—åˆ† - å¹³å‡: {statistics.mean(weighted_scores):.2f}, "
            f"ä¸­ä½æ•°: {statistics.median(weighted_scores):.2f}, "
            f"èŒƒå›´: [{min(weighted_scores):.2f}, {max(weighted_scores):.2f}]"
        )


if __name__ == "__main__":
    main()
